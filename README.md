This is the artifact for submission 'When in Doubt Throw It out: Building on Confident Learning for Vulnerability Detection'

first apply cl (/clmethods/clean.py) to get the out-of-sample predictions for the training samples 

then run the pruning script (/pruneandtest/prune.py) to get the mislabeled-cleared and confusing-cleared training sets

run the testing script (/pruneandtest/test.py) to see the performances

results in table 3 can be found in folder /clmethods 

results in table 4 can be found in folder /pruneandtest

figure 3 and 4 can be generated by the script 'plot-fig1.py' 'plot-fig2.py'
